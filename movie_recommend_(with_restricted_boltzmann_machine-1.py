# -*- coding: utf-8 -*-
"""Copy of Movie Recommend (with Restricted Boltzmann Machine

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mg3WQ9FBItzlYvIWWzk6rR26ee3EGwpR
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.autograd import Variable
import os

# Paths to datasets
train_file_path = os.path.join('/u1.base')
test_file_path = os.path.join('/u1.test')

# Load data
def load_data(file_path):
    data = pd.read_csv(file_path, delimiter='\t')
    data = np.array(data, dtype='int')
    return data

training_set = load_data(train_file_path)
test_set = load_data(test_file_path)

# Get number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

# Convert data into arrays with users in lines and movies in columns
def convert(data, nb_users, nb_movies):
    new_data = []
    for id_users in range(1, nb_users + 1):
        id_movies = data[:, 1][data[:, 0] == id_users]
        id_ratings = data[:, 2][data[:, 0] == id_users]
        ratings = np.zeros(nb_movies)
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

training_set = convert(training_set, nb_users, nb_movies)
test_set = convert(test_set, nb_users, nb_movies)

# Convert data to Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Convert ratings to binary
def binarize_data(data):
    data[data == 0] = -1
    data[data == 1] = 0
    data[data == 2] = 0
    data[data >= 3] = 1

binarize_data(training_set)
binarize_data(test_set)

# Define the RBM class
class RBM():
    def __init__(self, visible_node_count, hidden_node_count):
        self.weights = torch.randn(hidden_node_count, visible_node_count)
        self.hidden_node_bias = torch.randn(1, hidden_node_count)
        self.visible_node_bias = torch.randn(1, visible_node_count)

    def sample_hidden_nodes(self, x_visible_node_input_values):
        weighted_input = torch.mm(x_visible_node_input_values, self.weights.t())
        values_for_activation = weighted_input + self.hidden_node_bias.expand_as(weighted_input)
        activation_probabilities = torch.sigmoid(values_for_activation)
        return activation_probabilities, torch.bernoulli(activation_probabilities)

    def sample_visible_nodes(self, y_hidden_node_input_values):
        weighted_input = torch.mm(y_hidden_node_input_values, self.weights)
        values_for_activation = weighted_input + self.visible_node_bias.expand_as(weighted_input)
        activation_probabilities = torch.sigmoid(values_for_activation)
        return activation_probabilities, torch.bernoulli(activation_probabilities)

    def train(self, v_0, v_k, p_h_0, p_h_k):
        self.weights += (torch.mm(v_0.t(), p_h_0) - torch.mm(v_k.t(), p_h_k)).t()
        self.visible_node_bias += torch.sum((v_0 - v_k), 0)
        self.hidden_node_bias += torch.sum((p_h_0 - p_h_k), 0)

    def predict(self, x):
        _, h = self.sample_hidden_nodes(x)
        _, v = self.sample_visible_nodes(h)
        return v

# Training the RBM
visible_node_count = len(training_set[0])
hidden_node_count = 100
batch_size = 100
rbm = RBM(visible_node_count, hidden_node_count)

number_of_epochs = 50
for epoch in range(1, number_of_epochs + 1):
    train_loss = 0
    s = 0.
    for user_id in range(0, nb_users - batch_size, batch_size):
        v_k = training_set[user_id:user_id + batch_size]
        v_0 = training_set[user_id:user_id + batch_size]
        p_h_0, _ = rbm.sample_hidden_nodes(v_0)
        for k in range(10):
            _, h_k = rbm.sample_hidden_nodes(v_k)
            _, v_k = rbm.sample_visible_nodes(h_k)
            v_k[v_0 < 0] = v_0[v_0 < 0]
        p_h_k, _ = rbm.sample_hidden_nodes(v_k)
        rbm.train(v_0, v_k, p_h_0, p_h_k)
        train_loss += torch.mean(torch.abs(v_0[v_0 >= 0] - v_k[v_0 >= 0]))
        s += 1.
    print(f'epoch: {epoch} loss: {train_loss/s}')

# Testing the RBM
test_loss = 0
s = 0.
for id_user in range(nb_users):
    v = training_set[id_user:id_user + 1]
    vt = test_set[id_user:id_user + 1]
    if len(vt[vt >= 0]) > 0:
        _, h = rbm.sample_hidden_nodes(v)
        _, v = rbm.sample_visible_nodes(h)
        test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))
        s += 1.
print(f'test loss: {test_loss/s}')

# Predict movie recommendation for a given user ID
def recommend_movies(user_id, test_set, rbm):
    user_input = Variable(test_set[user_id - 1]).unsqueeze(0)
    output = rbm.predict(user_input)
    return output

user_id = 23
recommended_movies = recommend_movies(user_id, test_set, rbm)
print(recommended_movies)

# Create recommendation set for each user
def create_recommendations(training_set, rbm, nb_users):
    reco_list = []
    for user in range(1, nb_users + 1):
        seen = training_set[user - 1]
        seenlist = [i for i, rating in enumerate(seen) if rating > 0]
        z = Variable(training_set[user - 1]).unsqueeze(0)
        output = rbm.predict(z)
        output_numpy = output.detach().numpy()[0]
        for movie_id, rating in enumerate(output_numpy):
            if rating >= 1 and movie_id not in seenlist:
                reco_list.append({'UserId': user, 'MovieId': movie_id + 1})
    reco = pd.DataFrame(reco_list)
    return reco

recommendations = create_recommendations(training_set, rbm, nb_users)
print(recommendations)

import pandas as pd
import torch
from torch.autograd import Variable

# Load movie names
movies_df = pd.read_csv('/u.item', delimiter='|', encoding='latin-1', usecols=[0, 1], names=['MovieID', 'Title'])

# Function to get movie names
def get_movie_names(movie_ids, movies_df):
    return movies_df[movies_df['MovieID'].isin(movie_ids)]['Title'].tolist()

# Function to recommend movies for a user
def recommend_movies(user_id, test_set, rbm, movies_df, top_n=3):
    user_input = Variable(test_set[user_id - 1]).unsqueeze(0)
    output = rbm.predict(user_input)
    output_numpy = output.detach().numpy()[0]
    # Get movie ratings and IDs
    movie_ratings = [(movie_id + 1, rating) for movie_id, rating in enumerate(output_numpy) if rating >= 1]
    # Sort movies by rating in descending order
    movie_ratings = sorted(movie_ratings, key=lambda x: x[1], reverse=True)
    # Get the top N recommended movie IDs
    recommended_movie_ids = [movie_id for movie_id, rating in movie_ratings[:top_n]]
    recommended_movies = get_movie_names(recommended_movie_ids, movies_df)
    return recommended_movies

# Get top 3 recommendations for a specific user
user_id = 31
recommended_movies = recommend_movies(user_id, test_set, rbm, movies_df, top_n=3)
print(f"Top 3 recommended movies for user {user_id}:")
for movie in recommended_movies:
    print(movie)

import numpy as np
import pandas as pd
import torch
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Function to evaluate the model
def evaluate_model(test_set, rbm, threshold=0.5):
    test_loss = 0
    s = 0.
    all_predictions = []
    all_true_ratings = []
    for id_user in range(nb_users):
        v = training_set[id_user:id_user + 1]
        vt = test_set[id_user:id_user + 1]
        if len(vt[vt >= 0]) > 0:
            _, h = rbm.sample_hidden_nodes(v)
            _, v = rbm.sample_visible_nodes(h)
            test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))
            s += 1.
            all_predictions.append(v[vt >= 0].detach().numpy())
            all_true_ratings.append(vt[vt >= 0].detach().numpy())
    print('Test loss: ' + str(test_loss / s))
    return np.concatenate(all_predictions), np.concatenate(all_true_ratings)

# Evaluate the model
predictions, true_ratings = evaluate_model(test_set, rbm)

# Thresholding predictions
threshold = 0.5  # You can adjust this threshold
binary_predictions = (predictions >= threshold).astype(int)
true_ratings_binary = (true_ratings >= threshold).astype(int)

# Calculate accuracy
accuracy = accuracy_score(true_ratings_binary, binary_predictions)
print(f'Accuracy: {accuracy*100:.4f}')

# Generate confusion matrix
conf_matrix = confusion_matrix(true_ratings_binary, binary_predictions)
print('Confusion Matrix:')
print(conf_matrix)

# Classification report
class_report = classification_report(true_ratings_binary, binary_predictions, target_names=['Not Liked', 'Liked'])
print('Classification Report:')
print(class_report)

# Analyze the results
def analyze_results(predictions, true_ratings_binary):
    results = pd.DataFrame({'True Rating': true_ratings_binary, 'Predicted Rating': binary_predictions})
    grouped_results = results.groupby(['True Rating', 'Predicted Rating']).size().unstack(fill_value=0)
    return grouped_results

# Display analytical results
analytical_results = analyze_results(binary_predictions, true_ratings_binary)
print('Analytical Results:')
print(analytical_results)

# Visualization
def visualize_results(true_ratings_binary, binary_predictions, conf_matrix):
    # Confusion Matrix Heatmap
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Liked', 'Liked'], yticklabels=['Not Liked', 'Liked'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Distribution of True and Predicted Ratings
    plt.figure(figsize=(10, 7))
    sns.histplot(true_ratings_binary, color='blue', label='True Ratings', kde=True, stat="density", linewidth=0)
    sns.histplot(binary_predictions, color='red', label='Predicted Ratings', kde=True, stat="density", linewidth=0)
    plt.legend()
    plt.title('Distribution of True and Predicted Ratings')
    plt.show()

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(true_ratings_binary, predictions)

    plt.figure(figsize=(10, 7))
    plt.plot(recall, precision, marker='.')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(true_ratings_binary, predictions)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(10, 7))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

# Visualize the results
visualize_results(true_ratings_binary, binary_predictions, conf_matrix)

import numpy as np
import pandas as pd
import torch
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Function to evaluate the model
def evaluate_model(test_set, rbm, threshold=0.5):
    test_loss = 0
    s = 0.
    all_predictions = []
    all_true_ratings = []
    for id_user in range(nb_users):
        v = training_set[id_user:id_user + 1]
        vt = test_set[id_user:id_user + 1]
        if len(vt[vt >= 0]) > 0:
            _, h = rbm.sample_hidden_nodes(v)
            _, v = rbm.sample_visible_nodes(h)
            test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))
            s += 1.
            all_predictions.append(v[vt >= 0].detach().numpy())
            all_true_ratings.append(vt[vt >= 0].detach().numpy())
    print('Test loss: ' + str(test_loss / s))
    return np.concatenate(all_predictions), np.concatenate(all_true_ratings)

# Evaluate the model
predictions, true_ratings = evaluate_model(test_set, rbm)

# Thresholding predictions
threshold = 0.5  # You can adjust this threshold
binary_predictions = (predictions >= threshold).astype(int)
true_ratings_binary = (true_ratings >= threshold).astype(int)

# Calculate accuracy
accuracy = accuracy_score(true_ratings_binary, binary_predictions)
print(f'Accuracy: {accuracy*100:.4f}')

# Generate confusion matrix
conf_matrix = confusion_matrix(true_ratings_binary, binary_predictions)
print('Confusion Matrix:')
print(conf_matrix)

# Classification report
class_report = classification_report(true_ratings_binary, binary_predictions, target_names=['Not Liked', 'Liked'])
print('Classification Report:')
print(class_report)

# Function to plot pie charts
def plot_pie_chart(labels, sizes, title):
    plt.figure(figsize=(8, 8))
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.title(title)
    plt.show()

# Plotting pie chart for true ratings
true_labels = ['Not Liked', 'Liked']
true_sizes = [np.sum(true_ratings_binary == 0), np.sum(true_ratings_binary == 1)]
plot_pie_chart(true_labels, true_sizes, 'True Ratings Distribution')

# Plotting pie chart for predicted ratings
predicted_labels = ['Not Liked', 'Liked']
predicted_sizes = [np.sum(binary_predictions == 0), np.sum(binary_predictions == 1)]
plot_pie_chart(predicted_labels, predicted_sizes, 'Predicted Ratings Distribution')

# Plotting pie chart for confusion matrix
conf_labels = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']
conf_sizes = [conf_matrix[0, 0], conf_matrix[0, 1], conf_matrix[1, 0], conf_matrix[1, 1]]
plot_pie_chart(conf_labels, conf_sizes, 'Confusion Matrix Distribution')

import matplotlib.pyplot as plt

# Assuming `epochs` and `losses` are already defined
epochs = list(range(1, 51))
losses = [
    0.347933828830719, 0.2344128042459488, 0.24997790157794952, 0.24614262580871582,
    0.25117945671081543, 0.24648547172546387, 0.24806226789951324, 0.24742282927036285,
    0.24958494305610657, 0.24545611441135406, 0.24767443537712097, 0.24517996609210968,
    0.24452728033065796, 0.2457459270954132, 0.2470356673002243, 0.2442876249551773,
    0.24664442241191864, 0.246909037232399, 0.24370677769184113, 0.24720165133476257,
    0.24365456402301788, 0.2476692795753479, 0.2434154599905014, 0.24623309075832367,
    0.24343140423297882, 0.24440884590148926, 0.24714750051498413, 0.24368412792682648,
    0.24663424491882324, 0.24467331171035767, 0.247164785861969, 0.2426561713218689,
    0.24798832833766937, 0.244009330868721, 0.24540740251541138, 0.2470608502626419,
    0.24297849833965302, 0.2457677274942398, 0.24576398730278015, 0.247294083237648,
    0.24493278563022614, 0.24509644508361816, 0.24698859453201294, 0.24548377096652985,
    0.2465936839580536, 0.24288439750671387, 0.2455139458179474, 0.24518926441669464,
    0.24685978889465332, 0.24402110278606415
]

plt.figure(figsize=(10, 6))
plt.plot(epochs, losses, label='Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

# Assuming `true_ratings` and `predicted_probabilities` are already defined
precision, recall, _ = precision_recall_curve(true_ratings, predicted_probabilities)
average_precision = average_precision_score(true_ratings, predicted_probabilities)

plt.figure(figsize=(8, 6))
plt.step(recall, precision, where='post', color='b', alpha=0.6, label=f'Average Precision (AP) = {average_precision:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.grid(True)
plt.show()

"""# RECOMMENDATION MODEL 2:"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load MovieLens 100k dataset
url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'
columns = ['userId', 'movieId', 'rating', 'timestamp']
ratings_data = pd.read_csv(url, sep='\t', names=columns)

# Preprocess data
ratings_data['userId'] = ratings_data['userId'].astype('category').cat.codes.values
ratings_data['movieId'] = ratings_data['movieId'].astype('category').cat.codes.values

# Split data into training and testing sets
train, test = train_test_split(ratings_data, test_size=0.2, random_state=42)

# Build recommendation model
num_users = len(ratings_data['userId'].unique())
num_movies = len(ratings_data['movieId'].unique())
embedding_size = 50

user_input = tf.keras.layers.Input(shape=(1,), name='user_input')
movie_input = tf.keras.layers.Input(shape=(1,), name='movie_input')

user_embedding = tf.keras.layers.Embedding(input_dim=num_users, output_dim=embedding_size, input_length=1, name='user_embedding')(user_input)
movie_embedding = tf.keras.layers.Embedding(input_dim=num_movies, output_dim=embedding_size, input_length=1, name='movie_embedding')(movie_input)

user_flatten = tf.keras.layers.Flatten()(user_embedding)
movie_flatten = tf.keras.layers.Flatten()(movie_embedding)

prod = tf.keras.layers.Dot(axes=1)([user_flatten, movie_flatten])

model = tf.keras.Model(inputs=[user_input, movie_input], outputs=prod)
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit([train['userId'], train['movieId']], train['rating'],
                    batch_size=64, epochs=10,
                    validation_data=([test['userId'], test['movieId']], test['rating']))

# Visualize training history
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Example prediction
user_id = 0  # Example user ID
movie_id = 10  # Example movie ID
predicted_rating = model.predict([np.array([user_id]), np.array([movie_id])])
print(f'Predicted rating for user {user_id} and movie {movie_id}: {predicted_rating[0][0]}')

# Data Analysis
# Rating distribution
plt.hist(ratings_data['rating'], bins=5, edgecolor='black')
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Number of ratings per user
ratings_per_user = ratings_data.groupby('userId').size()
plt.hist(ratings_per_user, bins=30, edgecolor='black')
plt.title('Number of Ratings per User')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Users')
plt.show()

# Number of ratings per movie
ratings_per_movie = ratings_data.groupby('movieId').size()
plt.hist(ratings_per_movie, bins=30, edgecolor='black')
plt.title('Number of Ratings per Movie')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Movies')
plt.show()

# Average rating per user
avg_rating_per_user = ratings_data.groupby('userId')['rating'].mean()
plt.hist(avg_rating_per_user, bins=30, edgecolor='black')
plt.title('Average Rating per User')
plt.xlabel('Average Rating')
plt.ylabel('Number of Users')
plt.show()

# Average rating per movie
avg_rating_per_movie = ratings_data.groupby('movieId')['rating'].mean()
plt.hist(avg_rating_per_movie, bins=30, edgecolor='black')
plt.title('Average Rating per Movie')
plt.xlabel('Average Rating')
plt.ylabel('Number of Movies')
plt.show()

# Pie chart of ratings
rating_counts = ratings_data['rating'].value_counts()
plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Rating Distribution')
plt.show()

# Bar plot of top 10 most rated movies
top_10_movies = ratings_data['movieId'].value_counts().head(10)
plt.bar(top_10_movies.index, top_10_movies.values, color='skyblue')
plt.title('Top 10 Most Rated Movies')
plt.xlabel('Movie ID')
plt.ylabel('Number of Ratings')
plt.show()

# Bar plot of top 10 highest average rating movies
top_10_avg_rated_movies = ratings_data.groupby('movieId')['rating'].mean().nlargest(10)
plt.bar(top_10_avg_rated_movies.index, top_10_avg_rated_movies.values, color='lightgreen')
plt.title('Top 10 Highest Average Rating Movies')
plt.xlabel('Movie ID')
plt.ylabel('Average Rating')
plt.show()

"""## RECOMMENDATION MODEL 3:"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
def load_data(file_path):
    data = pd.read_csv(file_path, delimiter='\t', names=['userId', 'movieId', 'rating', 'timestamp'])
    return data

train_file_path = '/u1.base'
test_file_path = '/u1.test'

train_data = load_data(train_file_path)
test_data = load_data(test_file_path)

# Custom dataset class
class MovieLensDataset(Dataset):
    def __init__(self, data):
        self.users = torch.tensor(data['userId'].values, dtype=torch.long)
        self.movies = torch.tensor(data['movieId'].values, dtype=torch.long)
        self.ratings = torch.tensor(data['rating'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        return self.users[idx], self.movies[idx], self.ratings[idx]

train_dataset = MovieLensDataset(train_data)
test_dataset = MovieLensDataset(test_data)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Enhanced NCF model
class EnhancedNCF(nn.Module):
    def __init__(self, num_users, num_movies, embedding_dim=50):
        super(EnhancedNCF, self).__init__()
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim * 2, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.2)

    def forward(self, users, movies):
        user_emb = self.user_embedding(users)
        movie_emb = self.movie_embedding(movies)
        x = torch.cat([user_emb, movie_emb], dim=1)
        x = torch.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)
        x = torch.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = torch.sigmoid(self.fc4(x))
        return x

# Train the Enhanced NCF model
num_users = train_data['userId'].max() + 1
num_movies = train_data['movieId'].max() + 1

model = EnhancedNCF(num_users, num_movies)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 20
train_losses = []
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for users, movies, ratings in train_loader:
        optimizer.zero_grad()
        outputs = model(users, movies).squeeze()
        loss = criterion(outputs, ratings)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    avg_train_loss = train_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    print(f'Epoch {epoch+1}, Loss: {avg_train_loss}')

# Evaluate the Enhanced NCF model
model.eval()
test_loss = 0.0
with torch.no_grad():
    for users, movies, ratings in test_loader:
        outputs = model(users, movies).squeeze()
        loss = criterion(outputs, ratings)
        test_loss += loss.item()
avg_test_loss = test_loss / len(test_loader)
print(f'Test Loss: {avg_test_loss}')

# Function to recommend movies for a user
def recommend_movies_ncf(user_id, model, num_movies, top_n=3):
    user_input = torch.tensor([user_id] * num_movies, dtype=torch.long)
    movie_input = torch.tensor(list(range(num_movies)), dtype=torch.long)
    model.eval()
    with torch.no_grad():
        ratings = model(user_input, movie_input).squeeze()
    recommended_movie_ids = torch.argsort(ratings, descending=True)[:top_n].tolist()
    return recommended_movie_ids

# Load movie names
movies_df = pd.read_csv('/u.item', delimiter='|', encoding='latin-1', usecols=[0, 1], names=['MovieID', 'Title'])

# Function to get movie names
def get_movie_names(movie_ids, movies_df):
    return movies_df[movies_df['MovieID'].isin(movie_ids)]['Title'].tolist()

# Get top 3 recommendations for a specific user
user_id = 31
recommended_movie_ids = recommend_movies_ncf(user_id, model, num_movies, top_n=3)
recommended_movies = get_movie_names(recommended_movie_ids, movies_df)
print(f"Top 3 recommended movies for user {user_id} (Enhanced NCF):")
for movie in recommended_movies:
    print(movie)

# Plot training loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.show()

# Data analysis and visualization
def plot_rating_distribution(data):
    plt.figure(figsize=(10, 5))
    sns.histplot(data['rating'], bins=5, kde=True)
    plt.xlabel('Rating')
    plt.ylabel('Count')
    plt.title('Rating Distribution')
    plt.show()

def plot_user_movie_distribution(data):
    plt.figure(figsize=(10, 5))
    sns.histplot(data['userId'], bins=50, kde=True, color='blue', label='Users')
    sns.histplot(data['movieId'], bins=50, kde=True, color='red', label='Movies')
    plt.xlabel('ID')
    plt.ylabel('Count')
    plt.title('User and Movie ID Distribution')
    plt.legend()
    plt.show()

def plot_rating_count_per_user(data):
    user_rating_counts = data.groupby('userId').size()
    plt.figure(figsize=(10, 5))
    sns.histplot(user_rating_counts, bins=50, kde=True)
    plt.xlabel('Number of Ratings per User')
    plt.ylabel('Count')
    plt.title('Distribution of Ratings per User')
    plt.show()

# Plotting
plot_rating_distribution(train_data)
plot_user_movie_distribution(train_data)
plot_rating_count_per_user(train_data)

# Pie chart for rating proportions
def plot_rating_proportions(data):
    rating_counts = data['rating'].value_counts()
    plt.figure(figsize=(10, 5))
    plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)
    plt.title('Rating Proportions')
    plt.show()

plot_rating_proportions(train_data)